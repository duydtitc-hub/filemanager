import requests
from bs4 import BeautifulSoup
import re
import time
import os

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
OUTPUT_DIR = os.path.join(BASE_DIR, "outputs")
CACHE_DIR = os.path.join(BASE_DIR, "cache")
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(CACHE_DIR, exist_ok=True)
import hashlib
def url_hash(url: str) -> str:
    return hashlib.md5(url.encode("utf-8")).hexdigest()

import os
import re
import time
import requests
from bs4 import BeautifulSoup
from DiscordMethod import send_discord_message
CACHE_DIR = "cache"

def url_hash(u: str):
    import hashlib
    return hashlib.md5(u.encode("utf-8")).hexdigest()

def get_novel_text_laophatgia(url: str, delay: float = 1.0) -> str:
    """
    C√†o to√†n b·ªô n·ªôi dung truy·ªán t·ª´ laophatgia.net (d·∫°ng m·ªõi c√≥ danh s√°ch ch∆∞∆°ng trong <ul>).
    Bao g·ªìm t√≥m t·∫Øt ·ªü ƒë·∫ßu truy·ªán t·ª´ <div class="summary__content">
    """
    os.makedirs(CACHE_DIR, exist_ok=True)
    cache_file = os.path.join(CACHE_DIR, f"{url_hash(url)}.txt")
    cacheSumary = os.path.join(CACHE_DIR, f"sumary_{url_hash(url)}.txt")
 
    # üîπ D√πng cache n·∫øu c√≥
    if os.path.exists(cache_file):
        with open(cache_file, "r", encoding="utf-8") as f:
            content = f.read().strip()
            if content:
                sumary ="";
                send_discord_message(f"üì¶ D√πng cache: {cache_file}")
                if os.path.exists(cacheSumary):
                    with open(cacheSumary, "r", encoding="utf-8") as s:
                        sumary = s.read().strip()
                                             
                return content,sumary
    all_texts = ""

    def fetch_html(u):
        resp = requests.get(u, timeout=20)
        resp.encoding = "utf-8"
        resp.raise_for_status()
        return resp.text

    # üîπ L·∫•y HTML trang ch√≠nh
    html = fetch_html(url)
    soup = BeautifulSoup(html, "html.parser")

    # üîπ L·∫•y t√≥m t·∫Øt truy·ªán (summary)
    summary_container = soup.select_one("div.summary__content")
    paragraphs = []
    for p in summary_container.find_all("p"):
        # N·∫øu to√†n b·ªô <p> ch·ªâ ch·ª©a <strong>, b·ªè qua
        if p.find("strong") and len(p.get_text(strip=True)) == len(p.find("strong").get_text(strip=True)):
            continue
        text = p.get_text(" ", strip=True)
        if text:
            paragraphs.append(text)
    summary_text = "\n\n".join(paragraphs).strip()
    # üîπ L·∫•y danh s√°ch ch∆∞∆°ng
    chapter_items = soup.select("ul.main.version-chap.no-volumn li.wp-manga-chapter a")
    if not chapter_items:
        raise Exception("‚ùå Kh√¥ng t√¨m th·∫•y danh s√°ch ch∆∞∆°ng trong trang laophatgia.net")

    chapter_links = []
    for a in chapter_items:
        link = a.get("href")
        name = a.get_text(strip=True)
        if link and name:
            chapter_links.append({"name": name, "url": link})

    # Trang li·ªát k√™ ng∆∞·ª£c: Ch∆∞∆°ng m·ªõi nh·∫•t tr∆∞·ªõc, c·∫ßn ƒë·∫£o l·∫°i th·ª© t·ª±
    chapter_links.reverse()
    send_discord_message(f"üìö T·ªïng s·ªë ch∆∞∆°ng t√¨m th·∫•y: {len(chapter_links)}")

    # üîπ C√†o n·ªôi dung t·ª´ng ch∆∞∆°ng
    for i, chap in enumerate(chapter_links, start=1):
        try:
          
            chap_html = fetch_html(chap["url"])
            chap_soup = BeautifulSoup(chap_html, "html.parser")

            container = (
                chap_soup.select_one("div.reading-content div.text-left")
                or chap_soup.select_one("div.reading-content")
            )

            if not container:
                send_discord_message("‚ùå Kh√¥ng t√¨m th·∫•y n·ªôi dung trong ch∆∞∆°ng n√†y.")
                continue

            paragraphs = []
            for p in container.find_all(["p", "div"]):
                text = p.get_text(" ", strip=True)
                if not text:
                    continue
                # Ch·ªâ lo·∫°i b·ªè nh·ªØng c√¢u ch·ª©a URL/domain/watermark ‚Äî kh√¥ng v·ª©t c·∫£ <p>
                try:
                    sentences = re.split(r"(?<=[\.!?„ÄÇÔºÅÔºü])\s+", text)
                except Exception:
                    sentences = [text]

                kept = []
                for s in sentences:
                    if not s:
                        continue
                    # N·∫øu c√¢u ch·ªâ ch·ª©a s·ªë -> b·ªè
                    if re.fullmatch(r"\s*\d+[\.:\)\-]?\s*", s):
                        continue
                    # N·∫øu c√¢u b·∫Øt ƒë·∫ßu b·∫±ng s·ªë ƒë√°nh th·ª© t·ª± (v√≠ d·ª• "1. ..."), x√≥a ph·∫ßn ƒë√°nh s·ªë v√† gi·ªØ ph·∫ßn sau
                    s_stripped = re.sub(r"^\s*\d+[\.:\)\-]\s*", "", s)
                    if not s_stripped or not s_stripped.strip():
                        # n·∫øu sau khi b·ªè ti·ªÅn t·ªë kh√¥ng c√≤n n·ªôi dung th√¨ b·ªè c√¢u
                        continue
                    s = s_stripped
                    if re.search(r"laophatgia|https?://|ngu·ªìn|facebook|\.net|\.com|\.vn", s, re.I):
                        continue
                    kept.append(s.strip())

                if kept:
                    paragraphs.append(" ".join(kept))

            clean_text = "\n\n".join(paragraphs)
            # X√≥a "Ch∆∞∆°ng X" ·ªü ƒë·∫ßu d√≤ng ho·∫∑c ƒëo·∫°n
            clean_text = re.sub(r"(?im)^(ch∆∞∆°ng|chuong)\s*\d+[\.:‚Äì-]?\s*", "", clean_text, flags=re.MULTILINE)
            # X√≥a s·ªë ƒë∆°n ƒë·ªôc ·ªü ƒë·∫ßu d√≤ng (nh∆∞ "1." ho·∫∑c "123 ") - CH·ªà n·∫øu l√† d√≤ng ri√™ng
            clean_text = re.sub(r"(?m)^\s*\d+[\.:‚Äì-]\s*$", "", clean_text)
            # X√≥a t·∫•t c·∫£ k√Ω t·ª± ƒë·∫∑c bi·ªát, ch·ªâ gi·ªØ ch·ªØ c√°i (bao g·ªìm ti·∫øng Vi·ªát), s·ªë, kho·∫£ng tr·∫Øng v√† d·∫•u c√¢u c∆° b·∫£n
            clean_text = re.sub(r"[^\w\s.,!?();:\"'‚Ä¶‚Äî‚Äì-]", "", clean_text, flags=re.UNICODE)
            clean_text = re.sub(r"\n{2,}", "\n\n", clean_text).strip()

            all_texts += f"\n\n{clean_text}\n\n"
            time.sleep(delay)

        except Exception as e:
            send_discord_message(f"‚ùå L·ªói khi t·∫£i {chap['name']}: {e}")
            return ""
   
    # üîπ Ghi cache
    with open(cache_file, "w", encoding="utf-8") as f:
        f.write(all_texts.strip())
    with open(cacheSumary, "w", encoding="utf-8") as f:
        f.write(summary_text.strip())
    return all_texts.strip(),summary_text

def get_novel_text_vivutruyen(url: str, delay: float = 1.0) -> str:
    """
    C√†o to√†n b·ªô n·ªôi dung truy·ªán t·ª´ vivutruyen.net ho·∫∑c vivutruyen2.net.
    N·∫øu mi·ªÅn ƒë·∫ßu kh√¥ng c√≥ danh s√°ch ch∆∞∆°ng -> th·ª≠ mi·ªÅn c√≤n l·∫°i.
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                      "AppleWebKit/537.36 (KHTML, like Gecko) "
                      "Chrome/126.0 Safari/537.36",
        "Referer": "https://google.com/",
        "Accept-Language": "vi-VN,vi;q=0.9,en;q=0.8",
    }

    chapters = []
    van_an_text=''
    def fetch_html(u):
        """Return requests.Response or None after a few retries."""
        for _ in range(3):
            try:
                resp = requests.get(u, headers=headers, timeout=15, allow_redirects=True)
                resp.encoding = "utf-8"
                resp.raise_for_status()
                return resp
            except Exception as e:
                send_discord_message(f"‚ö†Ô∏è L·ªói t·∫£i {u}: {e}, th·ª≠ l·∫°i...")
                time.sleep(2)
        return None


    from urllib.parse import urljoin, urlparse

    def extract_chapters(soup, page_url: str = ""):
        """Tr√≠ch xu·∫•t danh s√°ch ch∆∞∆°ng t·ª´ HTML.
        1) C·∫•u tr√∫c uk-switcher (vivutruyen)
        2) Fallback c·∫•u tr√∫c grid trong el-content (novatruyen)
        """
        chapterlocal = []

        # 1) uk-switcher ti√™u chu·∫©n
        switcher = soup.select("ul.uk-switcher li div.list")
        if switcher:
            for block in switcher:
                for a in block.select("a.chap-title"):
                    chap_url = a.get("href")
                    chap_name = a.get_text(strip=True)
                    if chap_url and chap_name:
                        # Chu·∫©n h√≥a tuy·ªát ƒë·ªëi
                        abs_url = urljoin(page_url, chap_url) if page_url else chap_url
                        chapterlocal.append((chap_name, abs_url))
        else:
            send_discord_message("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y danh s√°ch ch∆∞∆°ng trong uk-switcher. Th·ª≠ fallback el-content...")

            # 2) Fallback: danh s√°ch ch∆∞∆°ng n·∫±m trong el-content c·ªßa tab (novatruyen)
            # V√≠ d·ª•: li.el-item[role="tabpanel"] > div.el-content.uk-panel.uk-margin-top ... a[href]
            containers = [
                "li.el-item[role=tabpanel] div.el-content.uk-panel.uk-margin-top",
                "div.el-content.uk-panel.uk-margin-top",
                "li.el-item .el-content",
                "div.el-content",
                "div.page-children.uk-grid",
            ]

            # Thu th·∫≠p t·∫•t c·∫£ container ph√π h·ª£p (kh√¥ng break s·ªõm ƒë·ªÉ tr√°nh b·ªè s√≥t)
            boxes = []
            for sel in containers:
                boxes.extend(soup.select(sel))

            if boxes:
                seen_urls = set()
                for box in boxes:
                    for a in box.select("a[href]"):
                        chap_url = a.get("href")
                        chap_name = a.get_text(" ", strip=True)
                        if not chap_url:
                            continue
                        # R√†ng bu·ªôc Nova: anchor ph·∫£i c√≥ ch·ªØ "ch∆∞∆°ng" trong text ƒë·ªÉ tr√°nh tr√πng div
                        name_l = (chap_name or "").lower()
                        if "ch∆∞∆°ng" not in name_l and "chuong" not in name_l:
                            continue
                        abs_url = urljoin(page_url, chap_url) if page_url else chap_url
                        if abs_url in seen_urls:
                            continue
                        seen_urls.add(abs_url)
                        chapterlocal.append((chap_name, abs_url))

        return chapterlocal

    def build_domain_variants(u):
        """T·∫°o danh s√°ch URL t∆∞∆°ng ·ª©ng tr√™n 3 domain: vivutruyen.net, vivutruyen2.net, novatruyen.com.
        B·∫£o to√†n path/slug, ƒë·ªïi host t∆∞∆°ng ·ª©ng. Tr·∫£ v·ªÅ danh s√°ch unique, gi·ªØ th·ª© t·ª±.
        """
        def replace_host(u0, host):
            m = re.match(r"^(https?://)([^/]+)(/.*)?$", u0)
            if not m:
                return u0
            scheme, _, rest = m.groups()
            rest = rest or "/"
            return f"{scheme}{host}{rest}"

        variants = []
        hosts = ["vivutruyen.net", "vivutruyen2.net", "novatruyen.com"]

        # N·∫øu URL thu·ªôc host n√†o th√¨ v·∫´n th√™m b·∫£n g·ªëc ƒë·∫ßu ti√™n
        variants.append(u)

        # Th√™m c√°c host c√≤n l·∫°i, gi·ªØ nguy√™n scheme + path
        for h in hosts:
            if h in u:
                continue
            v = replace_host(u, h)
            if v not in variants:
                variants.append(v)

        # N·∫øu ƒë·∫ßu v√†o l√† novatruyen, th√™m c·∫£ 2 vivu host
        # N·∫øu ƒë·∫ßu v√†o l√† vivu, ƒë·∫£m b·∫£o th√™m novatruyen
        return variants
    def extract_van_an(soup):
        """L·∫•y vƒÉn √°n n·∫øu c√≥."""
        # 1) ∆Øu ti√™n box chu·∫©n n·∫øu c√≥
        selectors = [
            "div.uk-card.uk-card-small div.noi-dung",   # box chu·∫©n
            "div.noi-dung",                              # fallback nh·∫π
        ]

        box = None
        for sel in selectors:
            box = soup.select_one(sel)
            if box:
                break

        # 2) N·∫øu kh√¥ng c√≥ box -> l·∫•y t·ª´ ph·∫ßn content gi·ªõi thi·ªáu nh∆∞ v√≠ d·ª• ng∆∞·ªùi d√πng ƒë∆∞a
        #   <div class="uk-panel ...">
        #       <h3 class="el-title ...">Gi·ªõi thi·ªáu truy·ªán ...</h3>
        #       <div class="el-content uk-panel uk-margin-top"> ... <p>...</p> ... </div>
        #   </div>
        if not box:
            box = (
                soup.select_one("div.uk-panel div.el-content.uk-panel.uk-margin-top")
                or soup.select_one("div.el-content.uk-panel.uk-margin-top")
                or soup.select_one("div.uk-panel .el-content")
                or soup.select_one("div.el-content")
            )

        if not box:
            return ""

        parts = []
        for p in box.find_all("p"):
            text = p.get_text(" ", strip=True)
            # Lo·∫°i b·ªè c√°c ƒëo·∫°n ch·ªâ l√† &nbsp; ho·∫∑c r·ªóng
            if not text or re.fullmatch(r"[\xa0\s]+", text):
                continue
            parts.append(text)

        return "\n\n".join(parts).strip()
    cache_file = os.path.join(CACHE_DIR, f"{url_hash(url)}.txt")
    cacheSumary = os.path.join(CACHE_DIR, f"sumary_{url_hash(url)}.txt")
    if os.path.exists(cache_file):
        with open(cache_file, "r", encoding="utf-8") as f:
            content = f.read().strip()
            if content:
                sumary ="";
                send_discord_message(f"üì¶ D√πng cache: {cache_file}")
                if os.path.exists(cacheSumary):
                    with open(cacheSumary, "r", encoding="utf-8") as s:
                        sumary = s.read().strip()
                       
                           
                return content,sumary
                
    
    # Helper: determine whether fetched page likely corresponds to the requested story/slug.
    def page_matches_requested(orig_url: str, resp: requests.Response, soup: BeautifulSoup) -> bool:
        try:
            orig_path = urlparse(orig_url).path.rstrip('/')
            orig_seg = [s for s in orig_path.split('/') if s]
            orig_slug = orig_seg[-1] if orig_seg else ''
            # If original was a chapter URL like chuong-1, use the parent segment as story slug
            if re.match(r'^(chuong|ch)[-_]?\d+', orig_slug, re.I) and len(orig_seg) >= 2:
                orig_slug = orig_seg[-2]
        except Exception:
            orig_slug = ''

        try:
            final_url = resp.url or ''
            final_path = urlparse(final_url).path.rstrip('/')
            final_seg = [s for s in final_path.split('/') if s]
            final_slug = final_seg[-1] if final_seg else ''
            if re.match(r'^(chuong|ch)[-_]?\d+', final_slug, re.I) and len(final_seg) >= 2:
                final_slug = final_seg[-2]
        except Exception:
            final_slug = ''

        # If we can't determine slugs, be permissive (avoid false positives)
        if not orig_slug or not final_slug:
            return True

        if orig_slug == final_slug:
            return True

        # Check canonical / og:url meta tags for a match to the original slug
        try:
            can = soup.select_one('link[rel=canonical]')
            can_url = can['href'] if can and can.has_attr('href') else ''
        except Exception:
            can_url = ''
        try:
            og = soup.select_one('meta[property="og:url"], meta[name="og:url"]')
            og_url = og['content'] if og and og.has_attr('content') else ''
        except Exception:
            og_url = ''

        if orig_slug and (orig_slug in can_url or orig_slug in og_url):
            return True

        # No strong evidence page matches requested story
        return False

    send_discord_message(f"üåê ƒêang t·∫£i danh s√°ch ch∆∞∆°ng t·ª´ c√°c domain li√™n quan...")
    for variant in build_domain_variants(url):
        resp = fetch_html(variant)
        if not resp:
            continue
        soup = BeautifulSoup(resp.text, "html.parser")
        # N·∫øu trang ƒë√£ redirect sang m·ªôt truy·ªán kh√°c, b·ªè qua bi·∫øn th·ªÉ ƒë√≥
        try:
            if not page_matches_requested(variant, resp, soup):
                send_discord_message(f"‚ö†Ô∏è C√≥ v·∫ª trang {variant} ƒë√£ redirect sang truy·ªán kh√°c ({resp.url}), b·ªè qua.")
                continue
        except Exception as e:
            # Khi c√≥ l·ªói trong check, log v√† ti·∫øp t·ª•c b√¨nh th∆∞·ªùng
            send_discord_message(f"‚ö†Ô∏è L·ªói khi ki·ªÉm tra redirect cho {variant}: {e}")

        # L·∫•y vƒÉn √°n n·∫øu ch∆∞a c√≥
        if not van_an_text:
            van_an_text = extract_van_an(soup)
        # Gom ch∆∞∆°ng
        cs = extract_chapters(soup, variant)
        if cs:
            chapters.extend(cs)
        else:
            send_discord_message(f"‚ÑπÔ∏è Kh√¥ng t√¨m th·∫•y ch∆∞∆°ng t·∫°i: {variant}")

    if not chapters:
        raise Exception("‚ùå Kh√¥ng t√¨m th·∫•y danh s√°ch ch∆∞∆°ng tr√™n c·∫£ hai mi·ªÅn.")
    chapters = [c for c in chapters if isinstance(c, (list, tuple)) and len(c) > 0 and c[0]]
    def extract_num(name):
        m = re.search(r"(\d+)", name)
        return int(m.group(1)) if m else 0
   
    # S·∫Øp x·∫øp tr∆∞·ªõc
    chapters.sort(key=lambda c: extract_num(c[0]))

    # L·ªçc b·ªè tr√πng
    unique = []
    seen = set()

    for c in chapters:
        num = extract_num(c[0])
        if num not in seen:
            unique.append(c)
            seen.add(num)
    chapters = unique
    send_discord_message(f"üìö T·ªïng s·ªë ch∆∞∆°ng t√¨m th·∫•y: {len(chapters)}")
   
    # Sort theo s·ªë ch∆∞∆°ng (v√¨ site th∆∞·ªùng ƒë·ªÉ ng∆∞·ª£c)
    


    # L·ªçc b·ªè tr√πng
  
    all_texts = ''

    for i, chap in enumerate(chapters, start=1):
        try:
          
            chap_resp = fetch_html(chap[1])
            if not chap_resp:
                send_discord_message(f"‚ö†Ô∏è Kh√¥ng t·∫£i ƒë∆∞·ª£c ch∆∞∆°ng: {chap[1]}")
                continue
            chap_soup = BeautifulSoup(chap_resp.text, "html.parser")

            # Th·ª≠ c√°c container ph·ªï bi·∫øn (th√™m c·∫•u tr√∫c trang Nova)
            container = (
                chap_soup.select_one("div.uk-width-1-1.reading")
                or chap_soup.select_one("div.reading-content")
                or chap_soup.select_one("article.uk-article")
                or chap_soup.select_one("div.content-reading")
                or chap_soup.select_one("div.uk-panel.uk-margin-remove-first-child.uk-margin-small.uk-text-justify div.el-content.uk-panel.uk-text-lead")
                or chap_soup.select_one("div.el-content.uk-panel.uk-text-lead")
            )
            if not container:
                raise Exception("‚ùå Kh√¥ng t√¨m th·∫•y n·ªôi dung truy·ªán trong trang vivutruyen")

            paragraphs = []
            for p in container.find_all("p"):
                # Kh√¥i ph·ª•c text ·∫©n trong span.fake
                for fake in p.select("span.fake[data-before]"):
                    real_text = fake.get("data-before", "").strip()
                    fake.replace_with(real_text)

                text = p.get_text(" ", strip=True)
                if not text:
                    continue
                # L·ªçc qu·∫£ng c√°o, text r√°c ‚Äî ch·ªâ lo·∫°i b·ªè nh·ªØng c√¢u ch·ª©a pattern, kh√¥ng v·ª©t c·∫£ <p>
                try:
                    sentences = re.split(r"(?<=[\.!?„ÄÇÔºÅÔºü])\s+", text)
                except Exception:
                    sentences = [text]

                kept = []
                for s in sentences:
                    if not s:
                        continue
                    # N·∫øu c√¢u ch·ªâ ch·ª©a s·ªë -> b·ªè
                    if re.fullmatch(r"\s*\d+[\.:\)\-]?\s*", s):
                        continue
                    # N·∫øu c√¢u b·∫Øt ƒë·∫ßu b·∫±ng s·ªë ƒë√°nh th·ª© t·ª± (v√≠ d·ª• "1. ..."), x√≥a ph·∫ßn ƒë√°nh s·ªë v√† gi·ªØ ph·∫ßn sau
                    s_stripped = re.sub(r"^\s*\d+[\.:\)\-]\s*", "", s)
                    if not s_stripped or not s_stripped.strip():
                        continue
                    s = s_stripped
                    if re.search(r"https?://|ngu·ªìn|facebook|novatruyen|\.net|\.com|\.vn", s, re.I):
                        continue
                    kept.append(s.strip())

                if kept:
                    paragraphs.append(" ".join(kept))
                                    
            full_text = "\n\n".join(paragraphs)
            full_text = re.sub(r"\n{2,}", "\n\n", full_text).strip()
            # X√≥a "Ch∆∞∆°ng X" ·ªü ƒë·∫ßu d√≤ng
            full_text = re.sub(r"(?im)^(ch∆∞∆°ng|chuong)\s*\d+[\.:‚Äì-]?\s*", "", full_text, flags=re.MULTILINE)
            # X√≥a s·ªë ƒë∆°n ƒë·ªôc ·ªü ƒë·∫ßu d√≤ng (d√≤ng ri√™ng)
            full_text = re.sub(r"(?m)^\s*\d+[\.\)\-]?\s*$", "", full_text)
            full_text = re.sub(r"\n{2,}", "\n\n", full_text).strip()
            all_texts += full_text + "\n\n"
          
            time.sleep(delay)
        except Exception as e:
            send_discord_message(f"‚ùå L·ªói khi t·∫£i {chap[0]}: {e}")
    
    with open(cache_file, "w", encoding="utf-8") as f:
        f.write(all_texts.strip())
    with open(cacheSumary, "w", encoding="utf-8") as f:
        f.write(van_an_text.strip())

    send_discord_message(f"‚úÖ Ho√†n t·∫•t, l∆∞u cache: {cache_file}")
    return all_texts.strip(),van_an_text

def get_novel_text_wattpad(url: str, delay: float = 1.0) -> tuple[str, str]:
    """
    L·∫•y ph·∫ßn m√¥ t·∫£ / vƒÉn √°n t·ª´ trang Wattpad (n·∫øu c√≥).
    Tr·∫£ v·ªÅ tuple (full_text, summary_text) ‚Äî v·ªõi summary_text l√† ph·∫ßn "sau ch·ªØ VƒÉn √°n" n·∫øu t√¨m th·∫•y.
    CH·ªà l∆∞u cache summary, KH√îNG l∆∞u cache n·ªôi dung truy·ªán (full_text).
    """
    os.makedirs(CACHE_DIR, exist_ok=True)
    cacheSumary = os.path.join(CACHE_DIR, f"sumary_{url_hash(url)}.txt")

    # D√πng cache summary n·∫øu c√≥
    if os.path.exists(cacheSumary):
        try:
            with open(cacheSumary, "r", encoding="utf-8") as s:
                sumary = s.read().strip()
                send_discord_message(f"üì¶ D√πng cache summary Wattpad: {cacheSumary}")
                # Tr·∫£ v·ªÅ r·ªóng cho full_text v√¨ Wattpad ch·ªâ c√≥ summary
                return "", sumary
        except Exception:
            pass

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0 Safari/537.36",
        "Referer": "https://google.com/",
        "Accept-Language": "vi-VN,vi;q=0.9,en;q=0.8",
    }

    try:
        resp = requests.get(url, headers=headers, timeout=20)
        resp.encoding = "utf-8"
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        desc = soup.select_one('div[itemprop="description"]') or soup.select_one('div[itemprop=description]')
        summary_text = ""
        if desc:
            # L·∫•y to√†n b·ªô text trong div
            desc_text = desc.get_text(" ", strip=True)
            # T√¨m v·ªã tr√≠ ch·ªØ 'vƒÉn √°n' (kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng)
            m = re.search(r"vƒÉn\s*√°n", desc_text, re.I)
            if m:
                # L·∫•y ph·∫ßn sau ch·ªØ 'VƒÉn √°n'
                idx = m.end()
                summary_text = desc_text[idx:].strip(" \n\r\t:‚Äì‚Äî-‚Äì‚Äî")
            else:
                # N·∫øu kh√¥ng th·∫•y c·ª•m 'VƒÉn √°n', tr·∫£ nguy√™n ph·∫ßn m√¥ t·∫£
                summary_text = desc_text.strip()
        else:
            # Fallback: th·ª≠ meta og:description
            meta = soup.select_one('meta[property="og:description"], meta[name="description"]')
            if meta and meta.has_attr("content"):
                summary_text = meta["content"].strip()

        # N·∫øu m√¥ t·∫£ ch·ª©a domain ho·∫∑c li√™n k·∫øt (v√≠ d·ª•: .net, .com, wattpad.net) -> coi nh∆∞ kh√¥ng c√≥ vƒÉn √°n
        if summary_text and re.search(r"\.net|\.com|wattpad\.net", summary_text, re.I):
            send_discord_message(f"‚ö†Ô∏è B·ªè qua vƒÉn √°n v√¨ ch·ª©a domain trong m√¥ t·∫£: {url}")
            summary_text = ""

    except Exception as e:
        send_discord_message(f"‚ùå L·ªói khi t·∫£i Wattpad {url}: {e}")
        return "", ""

    # Ghi cache CH·ªà cho summary (kh√¥ng cache full_text)
    try:
        with open(cacheSumary, "w", encoding="utf-8") as f:
            f.write(summary_text.strip())
    except Exception:
        pass

    return "", summary_text


def extract_first_chapter_link_from_html(html: str, base_url: str | None = None) -> str:
    """
    T·ª´ HTML trang ch√≠nh truy·ªán, t√¨m link Ch∆∞∆°ng 1.
    Tr·∫£ v·ªÅ URL ƒë·∫ßy ƒë·ªß (n·∫øu base_url ƒë∆∞·ª£c cung c·∫•p s·∫Ω d√πng urljoin ƒë·ªÉ chu·∫©n h√≥a),
    ho·∫∑c '' n·∫øu kh√¥ng t√¨m th·∫•y.
    Chi·∫øn l∆∞·ª£c:
    - T√¨m anchor c√≥ href ch·ª©a '/chuong-1' (case-insensitive)
    - N·∫øu kh√¥ng t√¨m th·∫•y, th·ª≠ c√°c selector ph·ªï bi·∫øn: .lstbtn a, .findchap a, a span.btn_truyen
    """
    from urllib.parse import urljoin
    soup = BeautifulSoup(html, "html.parser")

    # 1) T√¨m anchor href ch·ª©a 'chuong-1' ho·∫∑c '/chuong-1-'
    a = None
    for cand in soup.find_all('a', href=True):
        href = cand['href']
        if re.search(r"/chuong[-_]?1\b|/chuong-1-", href, re.I):
            a = cand
            break

    # 2) Fallback: t√¨m .lstbtn a c√≥ span.btn_truyen ho·∫∑c text 'Ch∆∞∆°ng 1'
    if not a:
        a = soup.select_one("div.lstbtn a[href*='chuong']") or soup.select_one("a:has(span.btn_truyen)")
        if a and not re.search(r"chuong[-_]?1", a.get('href', ''), re.I):
            # ki·ªÉm tra text
            txt = a.get_text(" ", strip=True)
            if not re.search(r"ch∆∞∆°ng\s*1|chuong\s*1", txt, re.I):
                a = None

    if not a:
        return ""

    href = a['href']
    if base_url:
        return urljoin(base_url, href)
    # n·∫øu href l√† absolute th√¨ tr·∫£ v·ªÅ nguy√™n v·∫πn
    if href.startswith('http'):
        return href
    return href


def get_first_chapter_link(url: str, timeout: int = 15) -> str:
    """T·∫£i trang `url` v√† tr·∫£ v·ªÅ link ch∆∞∆°ng 1 (chu·∫©n h√≥a), ho·∫∑c '' n·∫øu kh√¥ng t√¨m th·∫•y."""
    try:
        resp = requests.get(url, timeout=timeout)
        resp.encoding = 'utf-8'
        resp.raise_for_status()
    except Exception as e:
        send_discord_message(f"‚ùå L·ªói t·∫£i trang ch√≠nh ƒë·ªÉ t√¨m ch∆∞∆°ng 1: {e}")
        return ""

    link = extract_first_chapter_link_from_html(resp.text, base_url=resp.url)
    if link:
        send_discord_message(f"‚ÑπÔ∏è T√¨m ƒë∆∞·ª£c link Ch∆∞∆°ng 1: {link}")
    else:
        send_discord_message("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y link Ch∆∞∆°ng 1 trong trang ch√≠nh.")
    return link


def extract_chapter_content_and_next(html: str, base_url: str | None = None) -> tuple[str, str]:
    """
    T·ª´ HTML c·ªßa 1 trang ch∆∞∆°ng, tr·∫£ v·ªÅ (content_text, next_url).
    - content_text: vƒÉn b·∫£n trong <div class="truyen"> (chuy·ªÉn <br> th√†nh \n\n)
    - next_url: link ch∆∞∆°ng ti·∫øp theo (ƒë√£ chu·∫©n h√≥a theo base_url n·∫øu cung c·∫•p) ho·∫∑c ''
    """
    from urllib.parse import urljoin
    soup = BeautifulSoup(html, "html.parser")

    content_div = soup.select_one('div.truyen') or soup.select_one('div.reading-content') or soup.select_one('div.content-reading')
    content_text = ""
    if content_div:
        # Thay <br> b·∫±ng newline
        for br in content_div.find_all('br'):
            br.replace_with('\n')
        text = content_div.get_text('\n', strip=True)
        # Chu·∫©n h√≥a kho·∫£ng c√°ch
        content_text = re.sub(r"\n{2,}", "\n\n", text).strip()

    # T√¨m link 'Ch∆∞∆°ng ti·∫øp' trong div.chapter_control a.next ho·∫∑c a.next
    next_href = ""
    a_next = soup.select_one('div.chapter_control a.next') or soup.select_one('a.next')
    if a_next and a_next.has_attr('href'):
        next_href = a_next['href']
        if base_url:
            next_href = urljoin(base_url, next_href)

    return content_text, next_href


def extract_first_chapter_link_from_html(html: str, base_url: str | None = None) -> str:
    """
    Tr√≠ch xu·∫•t link ch∆∞∆°ng 1 t·ª´ ƒëo·∫°n HTML (v√≠ d·ª• nh∆∞ ph·∫ßn 'link ch∆∞∆°ng 1 n·∫±m ·ªü ƒë√¢y').
    N·∫øu href l√† ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi v√† base_url ƒë∆∞·ª£c cung c·∫•p, tr·∫£ v·ªÅ link tuy·ªát ƒë·ªëi.
    Tr·∫£ v·ªÅ chu·ªói r·ªóng n·∫øu kh√¥ng t√¨m th·∫•y.
    """
    try:
        soup = BeautifulSoup(html, "html.parser")
    except Exception:
        return ""

    for a in soup.find_all("a", href=True):
        txt = a.get_text(" ", strip=True)
        href = a["href"]
        # Ki·ªÉm tra text ch·ª©a 'Ch∆∞∆°ng 1' ho·∫∑c href ch·ª©a 'chuong-1' (kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng)
        if re.search(r"ch∆∞∆°ng\s*1", txt, re.I) or re.search(r"chuong[-_]1", href, re.I) or re.search(r"/chuong-1", href, re.I):
            # Chu·∫©n h√≥a href
            if href.startswith("//"):
                href = "https:" + href
            if base_url and href.startswith("/"):
                base = re.match(r"https?://[^/]+", base_url)
                if base:
                    href = base.group(0) + href
            elif base_url and not href.startswith("http"):
                href = base_url.rstrip("/") + "/" + href.lstrip("/")
            return href
    return ""


def get_first_chapter_link(page_url: str, timeout: int = 15) -> str:
    """
    T·∫£i trang `page_url` v√† tr·∫£ v·ªÅ link tuy·ªát ƒë·ªëi ƒë·∫øn ch∆∞∆°ng 1 n·∫øu t√¨m th·∫•y.
    S·ª≠ d·ª•ng heuristics: anchor text ch·ª©a 'Ch∆∞∆°ng 1' ho·∫∑c href ch·ª©a '/chuong-1' ho·∫∑c 'chuong-1'.
    Tr·∫£ v·ªÅ chu·ªói r·ªóng n·∫øu kh√¥ng t√¨m th·∫•y.
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0 Safari/537.36",
        "Referer": "https://google.com/",
    }
    try:
        resp = requests.get(page_url, headers=headers, timeout=timeout)
        resp.encoding = "utf-8"
        resp.raise_for_status()
        html = resp.text
    except Exception as e:
        send_discord_message(f"‚ö†Ô∏è L·ªói khi t·∫£i trang ƒë·ªÉ t√¨m ch∆∞∆°ng 1: {e}")
        return ""

    link = extract_first_chapter_link_from_html(html, base_url=page_url)
    if link:
        return link

    # Fallback: t√¨m tr√™n to√†n b·ªô c√°c a[href] n·∫øu ch∆∞a match
    try:
        soup = BeautifulSoup(html, "html.parser")
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if re.search(r"/chuong-1|chuong-1|chuong_1|ch-1|ch1", href, re.I):
                if href.startswith("/"):
                    base = re.match(r"https?://[^/]+", page_url)
                    if base:
                        return base.group(0) + href
                elif href.startswith("http"):
                    return href
                else:
                    return page_url.rstrip("/") + "/" + href.lstrip("/")
    except Exception:
        pass

    return ""


def extract_chapter_content_and_next(html: str, base_url: str | None = None) -> tuple[str, str | None]:
    """
    Tr√≠ch xu·∫•t n·ªôi dung ch∆∞∆°ng t·ª´ HTML v√† link 'Ch∆∞∆°ng ti·∫øp' n·∫øu c√≥ v√† c√≤n enabled.
    Tr·∫£ v·ªÅ (content_text, next_link) ‚Äî next_link = None n·∫øu kh√¥ng t√¨m th·∫•y ho·∫∑c link b·ªã disabled.
    """
    try:
        soup = BeautifulSoup(html, "html.parser")
    except Exception:
        return "", None

    # T√¨m container n·ªôi dung truy·ªán
    container = (
        soup.select_one("div.truyen")
        or soup.select_one("div.reading-content")
        or soup.select_one("div.reading")
        or soup.select_one("article")
    )

    if not container:
        return "", None

    # L·∫•y vƒÉn b·∫£n, chuy·ªÉn <br> th√†nh d√≤ng m·ªõi
    content_text = container.get_text("\n\n", strip=True)

    # T√¨m n√∫t 'Ch∆∞∆°ng ti·∫øp' v√† ki·ªÉm tra tr·∫°ng th√°i
    next_a = None
    # ∆Øu ti√™n c·∫•u tr√∫c c·ª• th·ªÉ
    nxt = soup.select_one("div.chapter_control a.next")
    if nxt:
        next_a = nxt
    else:
        # Fallback: t√¨m b·∫•t k·ª≥ anchor n√†o c√≥ class 'next' ho·∫∑c text 'Ch∆∞∆°ng ti·∫øp'
        for a in soup.find_all("a", href=True):
            cls = a.get("class") or []
            txt = a.get_text(" ", strip=True)
            if "next" in cls or re.search(r"ch∆∞∆°ng\s*ti·∫øp|ch∆∞∆°ng ti·∫øp|next", txt, re.I):
                next_a = a
                break

    if not next_a:
        return content_text, None

    href = next_a.get("href", "").strip()
    cls = next_a.get("class") or []

    # N·∫øu disabled theo class ho·∫∑c href r·ªóng / javascript / '#', coi nh∆∞ kh√¥ng c√≤n ch∆∞∆°ng ti·∫øp
    if any("disabled" == c or "disabled" in c for c in cls) or not href or href.startswith("javascript") or href == "#":
        return content_text, None

    # Chu·∫©n h√≥a th√†nh absolute URL n·∫øu c·∫ßn
    if href.startswith("//"):
        href = "https:" + href
    if base_url and href.startswith("/"):
        m = re.match(r"https?://[^/]+", base_url)
        if m:
            href = m.group(0) + href
    elif base_url and not href.startswith("http"):
        href = base_url.rstrip("/") + "/" + href.lstrip("/")

    return content_text, href


def crawl_chapters_until_disabled(start_page: str, delay: float = 1.0, max_chapters: int = 500) -> tuple[str, list[str]]:
    """
    B·∫Øt ƒë·∫ßu t·ª´ m·ªôt trang truy·ªán (c√≥ th·ªÉ l√† trang ch√≠nh ho·∫∑c link tr·ª±c ti·∫øp ƒë·∫øn ch∆∞∆°ng 1).
    N·∫øu start_page l√† trang ch√≠nh ch·ª©a link t·ªõi Ch∆∞∆°ng 1, h√†m s·∫Ω t·ª± t√¨m link ƒë·∫ßu ti√™n.
    Ti·∫øp t·ª•c c√†o c√°c ch∆∞∆°ng theo link 'Ch∆∞∆°ng ti·∫øp' cho ƒë·∫øn khi link b·ªã disabled (ho·∫∑c kh√¥ng c√≤n).

    Tr·∫£ v·ªÅ (full_text, chapter_urls) ‚Äî full_text l√† chu·ªói g·ªôp c√°c n·ªôi dung ch∆∞∆°ng, chapter_urls l√† danh s√°ch c√°c URL ƒë√£ l·∫•y.
    """
    # N·∫øu start_page l√† trang ch√≠nh (kh√¥ng ch·ª©a 'chuong-'), th·ª≠ t√¨m link ch∆∞∆°ng 1
    chap_url = start_page
    if not re.search(r"chuong[-_]?\d+", start_page, re.I):
        first = get_first_chapter_link(start_page)
        if first:
            chap_url = first

    collected = []
    urls = []
    count = 0
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0 Safari/537.36",
        "Referer": "https://google.com/",
    }

    while chap_url and count < max_chapters:
        try:
            resp = requests.get(chap_url, headers=headers, timeout=20)
            resp.encoding = "utf-8"
            resp.raise_for_status()
            html = resp.text
        except Exception as e:
            send_discord_message(f"‚ùå L·ªói khi t·∫£i ch∆∞∆°ng {chap_url}: {e}")
            break

        content, next_link = extract_chapter_content_and_next(html, base_url=chap_url)
        if content:
            # L√†m s·∫°ch n·ªôi dung gi·ªëng nh∆∞ c√°c method kh√°c
            # Lo·∫°i b·ªè c√°c d√≤ng ch·ª©a URL, domain, watermark
            lines = content.split("\n")
            clean_lines = []
            pattern = r"https?://|wattpad|\.net|\.com|\.vn|ngu·ªìn|facebook"
            for line in lines:
                line_stripped = line.strip()
                if not line_stripped:
                    continue
                # T√°ch line th√†nh c√¢u v√† ch·ªâ b·ªè nh·ªØng c√¢u ch·ª©a pattern
                try:
                    sentences = re.split(r"(?<=[\.!?„ÄÇÔºÅÔºü])\s+", line_stripped)
                except Exception:
                    sentences = [line_stripped]

                kept_sents = []
                for s in sentences:
                    if not s:
                        continue
                    # N·∫øu c√¢u ch·ªâ ch·ª©a s·ªë -> b·ªè
                    if re.fullmatch(r"\s*\d+[\.:\)\-]?\s*", s):
                        continue
                    # N·∫øu c√¢u b·∫Øt ƒë·∫ßu b·∫±ng s·ªë ƒë√°nh th·ª© t·ª± (v√≠ d·ª• "1. ..."), x√≥a ph·∫ßn ƒë√°nh s·ªë v√† gi·ªØ ph·∫ßn sau
                    s_stripped = re.sub(r"^\s*\d+[\.:\)\-]\s*", "", s)
                    if not s_stripped or not s_stripped.strip():
                        continue
                    s = s_stripped
                    if re.search(pattern, s, re.I):
                        continue
                    kept_sents.append(s.strip())

                if kept_sents:
                    clean_lines.append(" ".join(kept_sents))

            clean_content = "\n\n".join(clean_lines)
            # X√≥a "Ch∆∞∆°ng X" ·ªü ƒë·∫ßu d√≤ng
            clean_content = re.sub(r"(?im)^(ch∆∞∆°ng|chuong)\s*\d+[\.:‚Äì-]?\s*", "", clean_content, flags=re.MULTILINE)
            # X√≥a s·ªë ƒë∆°n ƒë·ªôc ·ªü ƒë·∫ßu d√≤ng (d√≤ng ri√™ng)
            clean_content = re.sub(r"(?m)^\s*\d+[\.:‚Äì-]\s*$", "", clean_content)
            # X√≥a t·∫•t c·∫£ k√Ω t·ª± ƒë·∫∑c bi·ªát, ch·ªâ gi·ªØ ch·ªØ c√°i (bao g·ªìm ti·∫øng Vi·ªát), s·ªë, kho·∫£ng tr·∫Øng v√† d·∫•u c√¢u c∆° b·∫£n
            clean_content = re.sub(r"[^\w\s.,!?();:\"'‚Ä¶‚Äî‚Äì-]", "", clean_content, flags=re.UNICODE)
            clean_content = re.sub(r"\n{2,}", "\n\n", clean_content).strip()
            
            if clean_content:
                collected.append(clean_content)
                urls.append(chap_url)
        else:
            send_discord_message(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y n·ªôi dung ch∆∞∆°ng t·∫°i {chap_url}")

        count += 1
        if not next_link:
            # H·∫øt ch∆∞∆°ng ho·∫∑c b·ªã disabled
            break

        # N·∫øu next_link gi·ªëng link hi·ªán t·∫°i, coi nh∆∞ ƒë√£ h·∫øt ch∆∞∆°ng (tr√°nh v√≤ng l·∫∑p)
        if next_link == chap_url:
            break

        chap_url = next_link
        time.sleep(delay)

    full = "\n\n".join(collected).strip()
    return full, urls


def get_wattpad_novel(url: str, delay: float = 1.0, max_chapters: int = 500) -> tuple[str, str, list[str]]:
    """
    T√≠ch h·ª£p: l·∫•y vƒÉn √°n t·ª´ trang Wattpad v√† c√†o l·∫ßn l∆∞·ª£t c√°c ch∆∞∆°ng theo 'Ch∆∞∆°ng ti·∫øp'
    cho ƒë·∫øn khi link 'Ch∆∞∆°ng ti·∫øp' b·ªã disabled.

    Tr·∫£ v·ªÅ (full_text, summary_text, chapter_urls)
    - full_text: CH·ªà n·ªôi dung c√°c ch∆∞∆°ng (KH√îNG bao g·ªìm vƒÉn √°n)
    - summary_text: ph·∫ßn t√≥m t·∫Øt / vƒÉn √°n thu·∫ßn (kh√¥ng k√®m header)
    - chapter_urls: danh s√°ch URL ch∆∞∆°ng ƒë√£ c√†o (theo th·ª© t·ª±)
    """
    os.makedirs(CACHE_DIR, exist_ok=True)
    cache_file = os.path.join(CACHE_DIR, f"{url_hash(url)}.txt")
    cacheSumary = os.path.join(CACHE_DIR, f"sumary_{url_hash(url)}.txt")

    # üîπ D√πng cache n·∫øu c√≥
    if os.path.exists(cache_file):
        with open(cache_file, "r", encoding="utf-8") as f:
            content = f.read().strip()
            if content:
                sumary = ""
                send_discord_message(f"üì¶ D√πng cache Wattpad: {cache_file}")
                if os.path.exists(cacheSumary):
                    with open(cacheSumary, "r", encoding="utf-8") as s:
                        sumary = s.read().strip()
                # Kh√¥ng tr·∫£ chapter_urls t·ª´ cache, ch·ªâ tr·∫£ n·ªôi dung
                return content, sumary, []

    try:
        # L·∫•y vƒÉn √°n (summary)
        _, summary_text = get_novel_text_wattpad(url, delay=delay)

        # T√¨m link ch∆∞∆°ng 1 (c√≥ th·ªÉ tr√™n trang ch√≠nh ho·∫∑c ƒë√£ c√≥ trong URL)
        first = get_first_chapter_link(url)
        if not first:
            send_discord_message(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y link Ch∆∞∆°ng 1 cho: {url}")
            # Tr·∫£ v·ªÅ ch·ªâ vƒÉn √°n n·∫øu c√≥
            return "", summary_text, []

        # C√†o c√°c ch∆∞∆°ng t·ª´ ch∆∞∆°ng 1 ƒë·∫øn khi 'next' disabled
        chapters_text, chapter_urls = crawl_chapters_until_disabled(first, delay=delay, max_chapters=max_chapters)

        # Ch·ªâ tr·∫£ n·ªôi dung ch∆∞∆°ng, kh√¥ng n·ªëi vƒÉn √°n
        full = chapters_text

        # üîπ Ghi cache
        with open(cache_file, "w", encoding="utf-8") as f:
            f.write(full.strip())
        with open(cacheSumary, "w", encoding="utf-8") as f:
            f.write(summary_text.strip())

        send_discord_message(f"‚úÖ Ho√†n t·∫•t Wattpad, l∆∞u cache: {cache_file}")
        return full.strip(), summary_text, chapter_urls
    except Exception as e:
        send_discord_message(f"‚ùå L·ªói get_wattpad_novel: {e}")
        return "", "", []


def extract_domain_structure(url): 
    """T·ª± nh·∫≠n bi·∫øt domain v√† ch·ªçn c·∫•u tr√∫c ph√π h·ª£p""" 
    domain = re.search(r"https?://([^/]+)/", url).group(1) 
    if "metruyenhot" in domain: 
        return {"content_selector": "div.chapter-c", "next_text": "Ti·∫øp"} 
    elif "truyenfull" in domain: 
        return {"content_selector": "div.chapter-c", "next_text": "Ch∆∞∆°ng ti·∫øp"} 
    else: return {"content_selector": "div.chapter", "next_text": "Next"}


def get_novel_text_wattpad_com(url: str, delay: float = 1.0, max_chapters: int = 500) -> tuple[str, str]:
    """
    X·ª≠ l√Ω ri√™ng cho domain wattpad.com (kh√°c v·ªõi wattpad.com.vn).
    Chi·∫øn l∆∞·ª£c:
    - L·∫•y summary b·∫±ng `get_novel_text_wattpad` (n√≥ ch·ªâ tr·∫£ summary cho trang Wattpad n·∫øu c√≥).
    - T√¨m link Ch∆∞∆°ng 1 b·∫±ng `get_first_chapter_link`. N·∫øu kh√¥ng c√≥, th·ª≠ bi·∫øn th·ªÉ mobile (m.wattpad.com).
    - D√πng `crawl_chapters_until_disabled` ƒë·ªÉ l·∫ßn l∆∞·ª£t l·∫•y ch∆∞∆°ng v·ªõi user-agent mobile.
    - Ghi cache t∆∞∆°ng t·ª± c√°c h√†m kh√°c.
    Tr·∫£ v·ªÅ (full_text, summary_text)
    """
    os.makedirs(CACHE_DIR, exist_ok=True)
    cache_file = os.path.join(CACHE_DIR, f"{url_hash(url)}.txt")
    cacheSumary = os.path.join(CACHE_DIR, f"sumary_{url_hash(url)}.txt")

    # D√πng cache n·∫øu c√≥
    if os.path.exists(cache_file):
        try:
            with open(cache_file, "r", encoding="utf-8") as f:
                content = f.read().strip()
                if content:
                    sumary = ""
                    send_discord_message(f"üì¶ D√πng cache Wattpad(.com): {cache_file}")
                    if os.path.exists(cacheSumary):
                        with open(cacheSumary, "r", encoding="utf-8") as s:
                            sumary = s.read().strip()
                    return content, sumary
        except Exception:
            pass

    # wattpad.com: kh√¥ng c·∫ßn summary, ch·ªâ l·∫•y n·ªôi dung ch∆∞∆°ng
    summary_text = ""

    # T√¨m link ch∆∞∆°ng 1
    first = url
    # N·∫øu kh√¥ng t√¨m th·∫•y, th·ª≠ bi·∫øn th·ªÉ mobile
    if not first:
        try:
            # chuy·ªÉn host sang m.wattpad.com
            first_candidate = re.sub(r"https?://(www\.)?wattpad\.com", "https://m.wattpad.com", url)
            first = get_first_chapter_link(first_candidate)
        except Exception:
            first = ""

    if not first:
        send_discord_message(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y link Ch∆∞∆°ng 1 cho (wattpad.com): {url}")
        # Tr·∫£ v·ªÅ ch·ªâ summary n·∫øu c√≥
        return "", (summary_text or "")

    # Crawl ch∆∞∆°ng cho wattpad.com ‚Äî s·ª≠ d·ª•ng container ƒë·∫∑c th√π
    headers = {
        "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1",
        "Referer": "https://google.com/",
    }

    collected = []
    urls = []
    chap_url = first
    # Normalize helper for URL comparisons (ignore trailing slash and lowercase host)
    from urllib.parse import urlparse, urljoin
    def _norm(u: str) -> str:
        try:
            p = urlparse(u)
            scheme = p.scheme or 'https'
            netloc = (p.netloc or '').lower()
            path = p.path or '/'
            # remove trailing slash for comparison
            path = path.rstrip('/')
            return f"{scheme}://{netloc}{path}"
        except Exception:
            return (u or '').rstrip('/').lower()

    norm_first = _norm(first)
    count = 0

    while chap_url and count < max_chapters:
        try:
            resp = requests.get(chap_url, headers=headers, timeout=20)
            resp.encoding = 'utf-8'
            resp.raise_for_status()
            soup = BeautifulSoup(resp.text, 'html.parser')
        except Exception as e:
            send_discord_message(f"‚ö†Ô∏è L·ªói t·∫£i ch∆∞∆°ng Wattpad(.com) {chap_url}: {e}")
            break

        # T√¨m container theo selector m√† b·∫°n cung c·∫•p
        container = soup.select_one("div.panel.panel-reading") or soup.select_one("div.panel-reading") or soup.select_one("div[dir=\"ltr\"]")
        chapter_text = ""
        if container:
            # ∆∞u ti√™n l·∫•y <pre> n·∫øu c√≥
            pref = container.select_one('pre') or container

            # Lo·∫°i b·ªè c√°c widget/komponent kh√¥ng c·∫ßn thi·∫øt
            for bad in pref.select('div.component-wrapper, button, .trinityAudioPlaceholder'):
                bad.decompose()

            # Thay <br> b·∫±ng newline
            for br in pref.find_all('br'):
                br.replace_with('\n')

            paras = []
            for p in pref.find_all('p'):
                text = p.get_text(' ', strip=True)
                if not text:
                    continue
                # L·ªçc r√°c: ch·ªâ lo·∫°i b·ªè nh·ªØng c√¢u/ƒëo·∫°n nh·ªè ch·ª©a URL/domain/wattpad,
                # kh√¥ng v·ª©t c·∫£ th·∫ª <p> khi ch·ªâ m·ªôt ph·∫ßn l√† qu·∫£ng c√°o.
                # T√°ch paragraph th√†nh c√¢u (ƒë∆°n gi·∫£n b·∫±ng regex) v√† gi·ªØ l·∫°i c√°c c√¢u kh√¥ng ch·ª©a pattern.
                try:
                    sentences = re.split(r"(?<=[\.!?„ÄÇÔºÅÔºü])\s+", text)
                except Exception:
                    sentences = [text]

                kept = []
                for s in sentences:
                    if not s:
                        continue
                    # N·∫øu c√¢u ch·ªâ ch·ª©a s·ªë -> b·ªè
                    if re.fullmatch(r"\s*\d+[\.:\)\-]?\s*", s):
                        continue
                    # N·∫øu c√¢u b·∫Øt ƒë·∫ßu b·∫±ng s·ªë ƒë√°nh th·ª© t·ª± (v√≠ d·ª• "1. ..."), x√≥a ph·∫ßn ƒë√°nh s·ªë v√† gi·ªØ ph·∫ßn sau
                    s_stripped = re.sub(r"^\s*\d+[\.:\)\-]\s*", "", s)
                    if not s_stripped or not s_stripped.strip():
                        continue
                    s = s_stripped
                    if re.search(r"https?://|wattpad|ngu·ªìn|facebook|\.com", s, re.I):
                        # ch·ªâ b·ªè c√¢u n√†y
                        continue
                    kept.append(s.strip())

                if kept:
                    paras.append(" ".join(kept))

            if not paras:
                # fallback: l·∫•y to√†n b·ªô text trong pre/container
                chapter_text = re.sub(r"\n{2,}", "\n\n", pref.get_text('\n', strip=True)).strip()
            else:
                chapter_text = "\n\n".join(paras).strip()
        else:
            send_discord_message(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y container ƒë·ªçc c·ªßa Wattpad t·∫°i: {chap_url}")

        if chapter_text:
            collected.append(chapter_text)
            urls.append(chap_url)

        # T√¨m link ch∆∞∆°ng ti·∫øp theo trong trang Wattpad
        next_link = None
        # 1) specific navigation container often used on Wattpad
        nav_a = soup.select_one('#story-part-navigation a') or soup.select_one('div.story-part-navigation a')
        if nav_a and nav_a.has_attr('href'):
            next_link = nav_a['href']
       

        # Normalize next_link
        if next_link:
            if next_link.startswith('//'):
                next_link = 'https:' + next_link
            if not next_link.startswith('http'):
                try:
                    next_link = urljoin(chap_url, next_link)
                except Exception:
                    next_link = chap_url.rstrip('/') + '/' + next_link.lstrip('/')

        # advance
        if not next_link:
            break

        # If next_link equals current chap_url or equals the initial first chapter, treat as end
        if next_link == chap_url or _norm(next_link) == norm_first:
            break
        chap_url = next_link
        count += 1
        time.sleep(delay)

    full_text = '\n\n'.join(collected).strip()

    # Ghi cache (ch·ªâ n·ªôi dung) ‚Äî kh√¥ng l∆∞u summary cho wattpad.com
    try:
        with open(cache_file, 'w', encoding='utf-8') as f:
            f.write(full_text)
    except Exception:
        pass

    send_discord_message(f"‚úÖ Ho√†n t·∫•t Wattpad(.com), l∆∞u cache: {cache_file}")
    return full_text, (summary_text or "")

def get_novel_text(url: str, include_summary: bool = True) -> tuple[str, str]:
    """
    L·∫•y to√†n b·ªô n·ªôi dung truy·ªán (MetruyenHot, TruyenFull, v.v.)
    - MetruyenHot: h·ªó tr·ª£ c·∫£ <p> text th∆∞·ªùng v√† <p> c√≥ text trong attribute l·∫°
    - T·ª± lo·∫°i watermark
    - D√πng cache
    Tr·∫£ v·ªÅ: (full_text, summary_text) - full_text CH·ªà ch·ª©a n·ªôi dung truy·ªán, KH√îNG c√≥ vƒÉn √°n
    """
    info = extract_domain_structure(url)
    base_url = re.match(r"https?://[^/]+", url).group(0)
    cache_file = os.path.join(CACHE_DIR, f"{url_hash(url)}.txt")
    cacheSumary = os.path.join(CACHE_DIR, f"sumary_{url_hash(url)}.txt")

    # D√πng cache n·∫øu c√≥
    if os.path.exists(cache_file):
        send_discord_message("üì¶ D√πng cache truy·ªán t·ª´ %s", cache_file)
        with open(cache_file, "r", encoding="utf-8") as f:
            content = f.read().strip()
            if content:
                # ƒê·ªçc cache vƒÉn √°n n·∫øu c√≥ (ch·ªâ khi caller y√™u c·∫ßu)
                sumary = ""
                if include_summary and os.path.exists(cacheSumary):
                    try:
                        with open(cacheSumary, "r", encoding="utf-8") as s:
                            sumary = s.read().strip()
                    except Exception:
                        sumary = ""
                return content, (sumary if include_summary else "")
            else:
                send_discord_message("‚ö†Ô∏è File cache r·ªóng, t·∫£i l·∫°i n·ªôi dung...")

    all_text = ""
    summary_text = ""
    # N·∫øu l√† TruyenFull: l·∫•y vƒÉn √°n t·ª´ trang ch√≠nh (lo·∫°i b·ªè /chuong-1/)
    try:
        domain_check = re.search(r"https?://([^/]+)/", url).group(1)
    except Exception:
        domain_check = ""

    if "truyenfull" in domain_check:
        try:
            main_url = re.sub(r"/chuong-\d+/?$", "/", url)
            if main_url == url and not main_url.endswith('/'):
                # v·∫´n th·ª≠ th√™m '/' ƒë·ªÉ ch·∫Øc ch·∫Øn
                main_url = url + "/"
            send_discord_message("üîé L·∫•y vƒÉn √°n t·ª´ trang ch√≠nh: %s", main_url)
            resp_main = requests.get(main_url, timeout=15)
            resp_main.encoding = "utf-8"
            soup_main = BeautifulSoup(resp_main.text, "lxml")
            desc = soup_main.select_one('div.desc-text.desc-text-full[itemprop="description"]') \
                   or soup_main.select_one('div.desc-text[itemprop="description"]') \
                   or soup_main.select_one('div.desc-text')

            if desc and include_summary:
                paras = []
                for p in desc.find_all(['p','div']):
                    t = p.get_text(" ", strip=True)
                    if not t or re.fullmatch(r"[\xa0\s]+", t):
                        continue
                    paras.append(t)
                summary_text = "\n\n".join(paras).strip()
        except Exception as e:
            send_discord_message("‚ö†Ô∏è Kh√¥ng l·∫•y ƒë∆∞·ª£c vƒÉn √°n TruyenFull: %s", e)
    chapter = 1

    while url:
      
        try:
            response = requests.get(url, timeout=15)
            response.encoding = "utf-8"
        except Exception as e:
            send_discord_message("‚ùå L·ªói t·∫£i trang %s: %s", url, e)
            break

        soup = BeautifulSoup(response.text, "lxml")

        # === X√≥a watermark & script ===
        for wm in soup.select("div.show-c, div.ads, script, style"):
            wm.decompose()

        domain = re.search(r"https?://([^/]+)/", url).group(1)
        clean_text = ""

        # === MetruyenHot ===
        if "metruyenhot" in domain:
            container = soup.select_one("div.book-list.full-story.content.chapter-c")
            if not container:
                send_discord_message("‚ùå Kh√¥ng t√¨m th·∫•y n·ªôi dung truy·ªán trong MetruyenHot")
                break

            paragraphs = []
            default_attrs = {"class", "style", "onmousedown", "onselectstart", "oncopy", "oncut"}

            for p in container.find_all("p"):
                text_content = ""

                # N·∫øu <p> c√≥ text tr·ª±c ti·∫øp
                if p.get_text(strip=True):
                    text_content = p.get_text(" ", strip=True)
                else:
                    # N·∫øu text n·∫±m trong attribute l·∫°
                    for attr, val in p.attrs.items():
                        if attr not in default_attrs and isinstance(val, str) and val.strip():
                            text_content = val.strip()
                            break

                # B·ªè watermark ho·∫∑c d√≤ng r√°c
                if text_content and not re.search(r"metruyen\s*hot", text_content, re.I):
                    paragraphs.append(text_content)

            clean_text = "\n\n".join(paragraphs)
        elif "laophatgia" in domain:
            return get_novel_text_laophatgia(url)
        elif "wattpad" in domain:
            # Distinguish between wattpad.com (use dedicated handler) and other Wattpad domains (e.g., wattpad.com.vn)
            try:
                if "wattpad.com" in domain:
                    full_text, summary_text = get_novel_text_wattpad_com(url, delay=1.0)
                    return full_text, summary_text
                else:
                    full_text, summary_text, _ = get_wattpad_novel(url, delay=1.0)
                    return full_text, summary_text
            except Exception as e:
                send_discord_message(f"‚ö†Ô∏è L·ªói khi x·ª≠ l√Ω Wattpad cho domain {domain}: {e}")
                return "", ""
        elif "vivutruyen" in domain or "vivutruyen2" in domain:

            return get_novel_text_vivutruyen(url)
        # === TruyenFull ho·∫∑c site kh√°c ===
        elif "truyenfull" in domain:
            content = soup.select_one(info["content_selector"])
            if not content:
                send_discord_message("‚ùå Kh√¥ng t√¨m th·∫•y n·ªôi dung t·∫°i %s", url)
                break

            # X√≥a c√°c watermark ho·∫∑c ph·∫ßn qu·∫£ng c√°o
            for wm in content.select("div.show-c, div.ads, script, style"):
                wm.decompose()

            clean_text = content.get_text("\n", strip=True)

    # === L√†m s·∫°ch n·ªôi dung ===
        # X√≥a "Ch∆∞∆°ng X" ·ªü ƒë·∫ßu d√≤ng
        clean_text = re.sub(r"(?im)^(ch∆∞∆°ng|chuong)\s*\d+[\.:‚Äì-]?\s*", "", clean_text, flags=re.MULTILINE)
        # X√≥a s·ªë ƒë∆°n ƒë·ªôc ·ªü ƒë·∫ßu d√≤ng (d√≤ng ri√™ng)
        clean_text = re.sub(r"(?m)^\s*\d+[\.:‚Äì-]\s*$", "", clean_text)
        clean_text = re.sub(r"\n{2,}", "\n\n", clean_text).strip()

        all_text += clean_text + "\n\n"

        # === X√°c ƒë·ªãnh link ch∆∞∆°ng ti·∫øp theo ===
        next_url = None
        if "truyenfull" in domain:
            next_link = soup.find("a", id="next_chap")
            if next_link and next_link.get("href"):
                next_url = next_link["href"]
        elif "metruyenhot" in domain:
            next_link = soup.find("a", attrs={"rel": "next"}) or \
                        soup.find("a", string=re.compile("Ti·∫øp", re.I))
            if next_link and next_link.get("href"):
                next_url = next_link["href"]

        # Fallback chung
        if not next_url:
            for a in soup.select("a"):
                href = a.get("href")
                if href and re.search(r"(ch∆∞∆°ng\s*ti·∫øp|ti·∫øp|next)", a.get_text(strip=True), re.I):
                    next_url = href
                    break

        # Chu·∫©n h√≥a URL
        if next_url and not next_url.startswith("javascript"):
            norm_next = next_url if next_url.startswith("http") else base_url + next_url
            # N·∫øu next gi·ªëng URL hi·ªán t·∫°i -> xem nh∆∞ ch∆∞∆°ng cu·ªëi
            if norm_next == url:
                send_discord_message("üö™ H·∫øt ch∆∞∆°ng t·∫°i: %s", url)
                url = None
            else:
                url = norm_next
                chapter += 1
        else:
            send_discord_message("üö™ H·∫øt ch∆∞∆°ng t·∫°i: %s", url)
            url = None
        # === Ghi cache t·ª´ng ch∆∞∆°ng (overwrite) === 
    with open(cache_file, "w", encoding="utf-8") as f: 
        f.write(all_text) 
    # Ghi cache summary (n·∫øu c√≥ v√† caller cho ph√©p)
    try:
        if include_summary:
            with open(cacheSumary, "w", encoding="utf-8") as s:
                s.write(summary_text or "")
    except Exception:
        pass

    return all_text.strip(), (summary_text if include_summary else "")


